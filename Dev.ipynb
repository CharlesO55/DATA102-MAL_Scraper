{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3a3102c-27c6-403a-8cf9-5f2123ee80f2",
   "metadata": {},
   "source": [
    "### Packages\n",
    "Uncomment and install if you don't have these yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b40bd8-2455-4d70-9172-b01789e37fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install pandas\\n!pip install beautifulsoup4\\n!pip install selenium\\n!pip install requests\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!pip install pandas\n",
    "!pip install beautifulsoup4\n",
    "!pip install selenium\n",
    "!pip install requests\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef32daa-77a9-476a-93a1-7854378571fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from enum import Enum\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e6ed7c-aff9-4242-a26f-98c8c02a45f3",
   "metadata": {},
   "source": [
    "# Some useful classes to prepare.\n",
    "\n",
    "## The scraper\n",
    "<li>Help correctly build URLS</li>\n",
    "<li>Pull raw HTML and generate soups</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d54725c-9181-40c3-ba29-db09b55f318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    class SEASONS(Enum):\n",
    "        FALL = 'fall'\n",
    "        SPRING = 'spring'\n",
    "        SUMMER = 'summer'\n",
    "        WINTER = 'winter'\n",
    "    \n",
    "    def build_URL_Seasons(year : str | int, \n",
    "                          season : SEASONS) -> str:\n",
    "\n",
    "        return f\"https://myanimelist.net/anime/season/{year}/{season.value}\"\n",
    "\n",
    "    \n",
    "    def delay(minimum = 5):\n",
    "        time.sleep(np.random.rand() * 10 + minimum)\n",
    "\n",
    "    def scrape(url : str) -> BeautifulSoup:\n",
    "        #include a delay per get() to avoid DDOS \n",
    "        Scraper.delay()\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        if response.status_code != requests.status_codes.codes.all_ok:\n",
    "            raise ConnectionError\n",
    "        \n",
    "        return BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a1b6d2-e0e9-4c18-8734-ebd57d737352",
   "metadata": {},
   "source": [
    "## Pages\n",
    "Abstract template where Pages must\n",
    "<li>Parse their soups accordingly</li>\n",
    "\n",
    "\n",
    "### What pages are we expecting?\n",
    "<li>Seasonal page listing all anime titles links.</li>\n",
    "<li>Details page containing an entry's info.</li>\n",
    "<li>Stats page listing all reviews.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f021de-cffa-4b91-9b7a-48c9c9d4b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapedPage(ABC):\n",
    "    @abstractmethod\n",
    "    def parse() -> dict:\n",
    "        return {}\n",
    "\n",
    "    def cleanString(item : str) -> str :\n",
    "        return re.sub(r'\\s+', ' ', item).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd84856-1a1b-41e0-8ee4-680c3fc26a5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m CONTINUE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m CONTINUE\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CONTINUE = False\n",
    "\n",
    "assert CONTINUE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc4c5c6-2ca3-477d-9a46-5a372f9e838f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# WebScraping MAL(myanimelist.net)\n",
    "\n",
    "### About MAL\n",
    "One of the most popular anime forums containing rankings, discussions, and information on anime media (movies, shows, manga).\n",
    "\n",
    "### To do: \n",
    "<ol>\n",
    "    <li>List all entries under a specified season and category.</li>\n",
    "    <li>Find each entry's article page and gather all relevant data.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32c974-d55e-4a89-b239-37d811f601a7",
   "metadata": {},
   "source": [
    "## Step 0: Inspect the Websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9851c0-a950-49bf-b162-bfe1523a41c2",
   "metadata": {},
   "source": [
    "### What we Know\n",
    "\n",
    "<ol>\n",
    "    <li>Base url: <a href=\"https://myanimelist.net/\">https://myanimelist.net/</a></li>\n",
    "    <img src=\"Images\\base_url.png\"/>\n",
    "    <li>Url format for seasonal listings: <b>{BASE URL}/anime/season/{YEAR}/{SEASON}</b></li>\n",
    "    <img src=\"Images\\seasonal_url.png\"/>\n",
    "    <li>Url format for article pages: <b>{BASE URL}/anime/{ID}/{NAME}</b></li>\n",
    "    <img src=\"Images\\article_url.png\"/>\n",
    "    <li>All webpages are <b>static</b></li>\n",
    "    <li>Default requests.get() functions without restrictions.</li>\n",
    "    <li>The site doesn't require login to access data.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628782cb-be03-46d8-b597-f23076bfefe3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Season Page"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61b7f6e3-c16d-4ee3-a58e-95a96e2ad052",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Inspecting the seasonal list page\n",
    "\n",
    "### The headers\n",
    ">We can navigate by clicking the headers to select a specified {YEAR} and {SEASON}.\n",
    "<br>However we already how the url formatting works hence we don't need to add complication with Selenium.\n",
    "\n",
    "><b>{BASE URL}/anime/season/{YEAR}/{SEASON}</b>\n",
    "\n",
    ">Try these:\n",
    ">><a href=\"https://myanimelist.net/anime/season/2024/fall\">2024 and Fall</a>\n",
    "<br><a href=\"https://myanimelist.net/anime/season/2018/summer\">2018 and Summer</a>\n",
    "\n",
    "<img src=\"Images\\seasonal_headers.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266a7c2-e08a-49b0-a998-1063268ee004",
   "metadata": {},
   "source": [
    "Let's try building the url.\n",
    "\n",
    "> YEAR : String\n",
    "> <br>SEASON : String {'fall', 'summer', 'speing', 'winter'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b48ac51-e932-427e-adb0-3fdf631247c4",
   "metadata": {},
   "source": [
    "```python\n",
    "SEASONS_URL = \"https://myanimelist.net/anime/season/\"\n",
    "YEAR = \"2024\"\n",
    "SEASON = \"spring\"\n",
    "\n",
    "FULL_SEASONS_URL = f'{SEASONS_URL}{YEAR}/{SEASON}'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b7c06-92d2-4d7d-9619-63c776c7972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2024\n",
    "SEASON = Scraper.SEASONS.SPRING\n",
    "\n",
    "FULL_SEASONS_URL = Scraper.build_URL_Seasons(YEAR, SEASON)\n",
    "print(FULL_SEASONS_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095e339-99a4-4fb4-9e77-e11dd33f6884",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### The entries\n",
    "\n",
    ">The entries are all listed even without scrolling thus proving it's a static page.\n",
    "><br>There's some information provided but it's still nothing compared to the article page.\n",
    "\n",
    ">What we'll need from this page is just\n",
    "><li>The title</li>\n",
    "><li>The link to the article</li>\n",
    "\n",
    "<img src=\"Images\\seasonal_entries.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d1b23d-c238-4254-becf-9528ba8a3413",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 1: Scrape the HTML\n",
    "\n",
    "Using requests.get(), pull the HTML data.\n",
    "\n",
    "Using BeautifulSoup, convert the HTML into a Soup wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a13b01-fbb2-496e-92b7-3db9a636bf06",
   "metadata": {},
   "source": [
    "```python\n",
    "response = requests.get(FULL_SEASONS_URL)\n",
    "\n",
    "#Check if there was an issue encountered while getting the HTML\n",
    "assert(response.status_code == requests.status_codes.codes.all_ok)\n",
    "\n",
    "#Convert the blob into the soup wrapper\n",
    "BeautifulSoup(response.text, \"html.parser\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc6a4c-2a4f-46d7-aef0-7b9088cba283",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonalSoup = Scraper.scrape(FULL_SEASONS_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cef1d-0692-4da0-bd35-e5d781c2e485",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(seasonalSoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29bb7c5-86bd-4d8e-b094-1a847fb8a0d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 2: Parse the Needed Data\n",
    "\n",
    "> Return to the entry...\n",
    "> <br>The only actual information we need is the title and its link.\n",
    "> <br>We don't need the other data or the parent div container.\n",
    "\n",
    "<img src='Images/link_title.png'/>\n",
    "\n",
    "> To extract that there's the following attributes...\n",
    "> <li>element_type : a</li>\n",
    "> <li>class_name : link-title</li>\n",
    "\n",
    "<img src='Images/link_title_source.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ba453-c7da-4f37-9fb9-5ed14d32efb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "link_titles = seasonalSoup.find_all(class_ ='link-title')\n",
    "link_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a172575b-2786-49a1-aca3-87e2ec25f348",
   "metadata": {},
   "source": [
    "### Extract data\n",
    "Get the text and the href."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7171a6-a9fc-40cb-b64f-64df096793ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Title':[a.text for a in link_titles], \n",
    "    'Link':[a.get('href') for a in link_titles]\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a1d85d-de76-46b3-9ef8-9840b4ed8655",
   "metadata": {},
   "source": [
    "## Wrap\n",
    "> Let's extend the `ScrapedPage` to cover all the steps for the `SeasonalPage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9395ea7-1c56-44ae-8cd6-dfe603e18300",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeasonPage(ScrapedPage):\n",
    "    def parse(soup : BeautifulSoup) -> dict:\n",
    "        link_titles = soup.find_all(class_ ='link-title')\n",
    "\n",
    "        return {\n",
    "            'Title':[a.text for a in link_titles], \n",
    "            'Link':[a.get('href') for a in link_titles]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73771975-b5f5-405b-bd7e-5db9a1ecb779",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Season = pd.DataFrame(SeasonPage.parse(seasonalSoup))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05fbd6-bbc3-4238-a3a0-2f3d2b2dc779",
   "metadata": {},
   "source": [
    "### Checkpoint\n",
    "Let's save it as a csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f71406a-07ce-4d82-8d76-dbc5bb1ae034",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'Season/{YEAR}_{SEASON.value}.csv'\n",
    "df_Season.to_csv(filename)\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b700bf-77b0-45e7-b0b3-1195b18cf68d",
   "metadata": {},
   "source": [
    "### End of Scraping Season Page\n",
    "\n",
    "We now have a complete list of anime titles and its article page.\n",
    "\n",
    "We will be repeating the same methodology onwards in the individual article pages where we Inspect, Scrape, then Parse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b9590-cfdc-457c-820f-86219db6b19d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Scraping Articles Page\n",
    "<img src='Images/article.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e89edf-7deb-4841-86cf-4cbf4707d930",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Details Page"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00efb9ab-6f1b-45c6-967a-b9f8c2217cc2",
   "metadata": {},
   "source": [
    "> We will primarily be scraping the details page.\n",
    "> This is because there are different pages/headers depending on media type.\n",
    "\n",
    "### Episode headers\n",
    "<img src='Images/article_episode_headers.png'/>\n",
    "\n",
    "### Movie headers\n",
    "<img src='Images/article_movie_headers.png'/>\n",
    "\n",
    "> Information we need are in the left panel.\n",
    "> <br>The rest are redundant.\n",
    "> <br>Note that there's also a js script for displaying more/less titles, but the title name still exists within the static context. \n",
    "\n",
    "<img src='Images/article_details_panel.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e697c-2ee6-4bbd-978b-5fcb5507a978",
   "metadata": {},
   "source": [
    "> Let's first get the HTML and read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6934b0-d188-4009-b831-250fa1ab5afb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "link = df.iloc[0].Link\n",
    "print(link)\n",
    "\n",
    "detailsSoup = Scraper.scrape(link)\n",
    "print(detailsSoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234f919b-cbba-41f6-91d1-8b561050e8a3",
   "metadata": {},
   "source": [
    "### Extract Panel Data\n",
    "> There are 2 ways to extract it.\n",
    "\n",
    "> The first is to find the parent div and extract its <b>children</b>.\n",
    "> <br><img src='Images/div_spaceit_pad.png'/>\n",
    "> <br>But this has issues where other unwanted divs also use this class name.\n",
    "\n",
    "> The other alternative is to find a unique class.<br>\n",
    "> Which is the one used in the label.<br>\n",
    "<img src='Images/span_dark_text.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe0de6-54b5-4130-8326-3e5051a16915",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unwanted/Redundant:')\n",
    "print(detailsSoup.find_all(class_='spaceit_pad')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf03614-0096-4a3a-9132-73d877d7b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailsSoup.find_all(class_='dark_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c7aa9-b42c-47c4-b722-c654cd145aaf",
   "metadata": {},
   "source": [
    "> To access the content values, ask the siblings.<br>\n",
    "> <img src='Images/dark_text_siblings.png'/>\n",
    "\n",
    "> Adjust accordingly as some siblings are just text,\n",
    "> but others can be a hyperlink or even lists of them.<br>\n",
    "> <img src='Images/siblings_hyperlinks.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ee1b1d-7406-428c-a92a-300b5de7123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "\n",
    "for d in detailsSoup.find_all(class_='dark_text'):\n",
    "    label = d.text.strip()\n",
    "    content = d.find_next_sibling(string=True).strip()\n",
    "    \n",
    "    if content == '':\n",
    "        content = [a.text.strip() for a in d.find_next_siblings('a')]\n",
    "\n",
    "    output[label] = content\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeaf7e2-c1ca-4c08-af64-80434e7798b9",
   "metadata": {},
   "source": [
    "### Fixing score and Additional Info\n",
    "> Score is still missing. This is because it's composed of more complex siblings.\n",
    "\n",
    "> Luckily it has a unique class name so use that instead.\n",
    "\n",
    "> In addition, let's get more data\n",
    "> <li>Scorers</li>\n",
    "> <li>Synopsis</li>\n",
    "> <li>Related_Entries</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519de61c-6080-49fd-839f-46d1910d1e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['Score:'] = detailsSoup.find('span', attrs={'itemprop':'ratingValue'}).text.strip()\n",
    "output['Scorers'] = detailsSoup.find('span', attrs={'itemprop':'ratingCount'}).text.strip()\n",
    "output['Synopsis'] = re.sub(r'\\s+', ' ', detailsSoup.find('p', attrs={'itemprop':'description'}).text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ae340-81c9-49e1-8dca-73951992462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['Related_Entries'] = [re.sub(r'\\s+', ' ', entry.text).strip() for entry in detailsSoup.find_all('div', attrs={'class':'entry borderClass'})]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d467a63-113b-429d-97d2-8124961b5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Details = pd.DataFrame([output])\n",
    "df_Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9635d396-aed8-451b-8245-e0582e19cfe8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Wrap for Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b8066-2ea1-4fed-b1ac-d5c806cce32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetailsPage(ScrapedPage):\n",
    "    def parse(soup : BeautifulSoup) -> dict:\n",
    "        output = {}\n",
    "\n",
    "        try:\n",
    "            DetailsPage.parseLeftPanel(soup, output)\n",
    "        except Exception as e:\n",
    "            print('Left Panel Failed')\n",
    "            \n",
    "        try:\n",
    "            DetailsPage.parseScore(soup, output)\n",
    "        except Exception as e:\n",
    "            print('Score Failed')\n",
    "                  \n",
    "        try:\n",
    "            DetailsPage.parseSynopsis(soup, output)\n",
    "        except Exception as e:\n",
    "            print('Synopsis Failed')\n",
    "\n",
    "        try:\n",
    "            DetailsPage.parseRelatedEntries(soup, output)\n",
    "        except Exception as e:\n",
    "            print('Related Entries Failed')\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def parseLeftPanel(soup : BeautifulSoup, output : dict) :\n",
    "        for d in soup.find_all(class_='dark_text'):\n",
    "            label = d.text.replace(':','').strip()\n",
    "            content = d.find_next_sibling(string=True).strip()\n",
    "            \n",
    "            if content == '':\n",
    "                content = [a.text.strip() for a in d.find_next_siblings('a')]\n",
    "        \n",
    "            output[label] = content\n",
    "\n",
    "    def parseScore(soup : BeautifulSoup, output : dict) :\n",
    "        e = soup.find('span', attrs={'itemprop':'ratingValue'})\n",
    "\n",
    "        if e is None:\n",
    "            print('No score')\n",
    "            raise ValueError\n",
    "        \n",
    "        output['Score'] = e.text.strip()\n",
    "        output['Scorers'] = soup.find('span', attrs={'itemprop':'ratingCount'}).text.strip()\n",
    "\n",
    "    def parseSynopsis(soup : BeautifulSoup, output : dict) :\n",
    "        output['Synopsis'] = DetailsPage.cleanString(soup.find('p', attrs={'itemprop':'description'}).text)\n",
    "\n",
    "    def parseRelatedEntries(soup : BeautifulSoup, output : dict) :\n",
    "        output['Related_Entries'] = [DetailsPage.cleanString(entry.text) for entry in soup.find_all('div', attrs={'class':'entry borderClass'})]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e408c-d5a5-4801-b184-e3fe8cdafe0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Testing\n",
    ">Run these with known missing values and see how effective error handling is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5071a43a-4843-4bae-a61c-1f692dde6814",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_TEST = df_Season.iloc[[np.random.randint(df_Season.shape[0]),np.random.randint(df_Season.shape[0]),-15,-1]]\n",
    "BATCH_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ba058-1bc0-4b7b-a81d-a0acfbdc4d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Details = None\n",
    "\n",
    "for _, row in BATCH_TEST.iterrows():\n",
    "    output = {}\n",
    "    newRow = None\n",
    "    \n",
    "    try:\n",
    "        soup = Scraper.scrape(row.Link)\n",
    "        output = DetailsPage.parse(soup)\n",
    "        output['Title'] = row.Title\n",
    "\n",
    "        newRow = pd.DataFrame([output])\n",
    "        \n",
    "        if df_Details is None:\n",
    "            df_Details = newRow\n",
    "        else:\n",
    "            df_Details = pd.concat([df_Details, newRow], ignore_index=True)\n",
    "        \n",
    "        print(row.Title)\n",
    "    except Exception as e:\n",
    "        print(f'ERROR: {row.Link} {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46171df7-0893-4225-bb78-ce1ed372ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Details.to_csv(f'Detail/{YEAR}_{SEASON.value}.csv')\n",
    "df_Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c664a4-0dd8-4631-9377-78f8d7ab07b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### End of Details Page\n",
    "Run the full for loop in the other notebook to observe final results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462134f3-e2b6-425a-8772-0b1c93d8010d",
   "metadata": {},
   "source": [
    "# Statistics Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb616f-661a-4b61-919b-6734e71407f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert CONTINUE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4225f8-06fc-47d2-bc22-bc3a32fe96ba",
   "metadata": {},
   "source": [
    "> The stats page composes of 3 parts.\n",
    "> <li>A summary of viewing status.</li>\n",
    "> <li>A histogram of user scores.</li>\n",
    "> <li>All score postings and viewing status</li>\n",
    "\n",
    "<img src='Images/stats_page.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa46a0-945d-4e9d-8dae-4afd179c1156",
   "metadata": {},
   "source": [
    "### URL\n",
    "> The pages increment at values of `75` but cannot >= `7500` <br>\n",
    "> In short, only the latest 7499 posts are provided.\n",
    "\n",
    "#### Stats page landing<br>\n",
    "> https://myanimelist.net/anime/9617/K-On_Movie/stats\n",
    "\n",
    "or\n",
    "\n",
    "> https://myanimelist.net/anime/9617/K-On_Movie/stats?show=0#members\n",
    "#### Next page\n",
    "> https://myanimelist.net/anime/9617/K-On_Movie/stats?show=75#members\n",
    "\n",
    "#### Exceeding last page generates an error response\n",
    "<img src='Images/stats_page_404.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efaa4d7-0789-4233-be8e-844c46f14395",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('https://myanimelist.net/anime/9617/K-On_Movie/stats?show=5000000000#members').status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab30e3ae-fe7c-47d9-bc30-189cf48c80d1",
   "metadata": {},
   "source": [
    "## Part 1. User viewership submissions\n",
    "> We will only be able to get partial data since it caps at 7499, but its data can be used in providing latest aggregates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26c6c03-e6a4-4b4f-8ad1-a72ae3bf59bb",
   "metadata": {},
   "source": [
    "<img src='Images/stats_viewershipsubmission.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7601254-d5fb-4585-9338-d25d2ad633aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticsSubmissionPage(ScrapedPage):\n",
    "    def parse(soup : BeautifulSoup) -> dict :\n",
    "        table = soup.find('table', attrs={'class':'table-recently-updated'})\n",
    "\n",
    "        rows = [[header.text for header in table.find('tr').find_all('td')]]\n",
    "        \n",
    "        \n",
    "        for tr in table.find_all('tr')[1:]:\n",
    "            row = [tr.find('a', attrs={'class':'word-break'}).text.strip()]\n",
    "            [row.append(td.text.strip()) for td in tr.find_all('td', attrs={'class':'borderClass ac'})]\n",
    "    \n",
    "            rows.append(row)\n",
    "        return rows\n",
    "\n",
    "    def parseAsDf(soup : BeautifulSoup) -> pd.DataFrame :\n",
    "        rows = StatisticsSubmissionPage.parse(soup)\n",
    "        return pd.DataFrame(rows[1:], columns=rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94aba6-1134-4f8a-9e86-079bd72123ca",
   "metadata": {},
   "source": [
    "### Table data for plain text\n",
    "\n",
    "<img src='Images/stats_submission_text.png'>\n",
    "\n",
    "### Table data for user name\n",
    "\n",
    "<img src='Images/stats_user.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11222ab6-3ff5-4d11-ac24-272a94b205d6",
   "metadata": {},
   "source": [
    "### Try parsing Frieren last page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cbc459-87fc-44b0-99d3-61faee199729",
   "metadata": {},
   "outputs": [],
   "source": [
    "LINK = 'https://myanimelist.net/anime/52991/Sousou_no_Frieren/stats?show=7425#members'\n",
    "statsSoup = BeautifulSoup(requests.get('https://myanimelist.net/anime/52991/Sousou_no_Frieren/stats?show=7425#members').text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22caf8c-c1e7-47a2-a21c-6a7361334079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = statsSoup.find('table', attrs={'class':'table-recently-updated'})\n",
    "\n",
    "rows = [[header.text for header in table.find('tr').find_all('td')]]\n",
    "\n",
    "\n",
    "for tr in table.find_all('tr')[1:]:\n",
    "    row = [tr.find('a', attrs={'class':'word-break'}).text.strip()]\n",
    "    [row.append(td.text.strip()) for td in tr.find_all('td', attrs={'class':'borderClass ac'})]\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132577e7-b10d-410c-bca3-efc52c0c9486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Statistics = StatisticsSubmissionPage.parseAsDf(statsSoup)\n",
    "df_Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34487a2f-48b9-4441-9908-dc2049bcaa56",
   "metadata": {},
   "source": [
    "### Parse the Entire KON User Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cceb75-2e14-433c-9339-e1758aaa9e78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LINK = 'https://myanimelist.net/anime/9617/K-On_Movie/'\n",
    "\n",
    "df_Statistics = None\n",
    "\n",
    "for i in np.arange(0, 75*100, 75):\n",
    "    try:\n",
    "        URL = f'{LINK}stats?show={i}#members'\n",
    "        print(URL)\n",
    "\n",
    "        statsSoup = Scraper.scrape(URL)\n",
    "\n",
    "        \n",
    "        if df_Statistics is None:\n",
    "            df_Statistics = StatisticsSubmissionPage.parseAsDf(statsSoup)\n",
    "        else:\n",
    "            df_Statistics = pd.concat([df_Statistics, StatisticsSubmissionPage.parseAsDf(statsSoup)], ignore_index=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'END {e}')\n",
    "        break\n",
    "\n",
    "filename = f'Statistics/{LINK.split('/')[-2]}_Submissions.csv'\n",
    "df_Statistics.to_csv(filename)\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5acf26-a162-47f0-b729-8646e4f0cc67",
   "metadata": {},
   "source": [
    "## Part 2. Historical Summary\n",
    "> Let's also get the full picture based on the summary stats provided."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "107b9e66-b2ee-4ead-9782-665bb3d50063",
   "metadata": {},
   "source": [
    "<img src='Images/stats_summary.png'>\n",
    "\n",
    "Get the Header then get the proceeding text elements with labels and values.\n",
    "\n",
    "<img src='Images/stats_darktext.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf3151-71db-4b12-8e59-52c1e59cf4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "H2_SUMMARY = statsSoup.find('h2', attrs={'id':'summary_stats'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db304405-55fb-471d-aec7-d622985c6948",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "\n",
    "for s in H2_SUMMARY.find_all_next('span', attrs={'class':'dark_text'}):\n",
    "    output[s.text.strip(':')] = s.find_next_sibling(string=True).strip()\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db2037-cf61-49d8-9b11-0ad278517913",
   "metadata": {},
   "source": [
    "<img src='Images/stats_scoretable.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ee62e-2c6e-42cd-9710-f3c5e76f1d63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TABLE_SCORE = statsSoup.find('table', attrs={'class':'score-stats'})\n",
    "TABLE_SCORE.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a753e-dde5-403d-adc6-4e8f60d1004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in TABLE_SCORE.find_all('tr'):\n",
    "    output[tr.select('.score-label')[0].text] = re.search(r'(\\d+)', tr.find('small').text).group(1)\n",
    "\n",
    "pd.DataFrame([output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf3b8b-7c64-4a31-806e-4cec6913c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticsSummaryPage(ScrapedPage):\n",
    "    def parse(soup : BeautifulSoup) -> dict :\n",
    "        output = {}\n",
    "\n",
    "        try:\n",
    "            StatisticsSummaryPage.parseSummary(soup, output)\n",
    "            StatisticsSummaryPage.parseScores(soup, output)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        finally:\n",
    "            return output\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def parseSummary(soup : BeautifulSoup, output : dict):\n",
    "        H2_SUMMARY = soup.find('h2', attrs={'id':'summary_stats'})\n",
    "\n",
    "        for s in H2_SUMMARY.find_all_next('span', attrs={'class':'dark_text'}):\n",
    "            output[s.text.strip(':')] = s.find_next_sibling(string=True).strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def parseScores(soup : BeautifulSoup, output : dict):\n",
    "        TABLE_SCORE = soup.find('table', attrs={'class':'score-stats'})\n",
    "        \n",
    "        for tr in TABLE_SCORE.find_all('tr'):\n",
    "            output[tr.select('.score-label')[0].text] = re.search(r'(\\d+)', tr.find('small').text).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7bb03b-4958-45b9-8fb0-9ab55a0f9d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = StatisticsSummaryPage.parse(statsSoup)\n",
    "output['Title'] = LINK.split('/')[-2]\n",
    "[output]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
